{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // Or use any other 2.x version here\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.5` // Or use any other 2.x version here\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add spark hive dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "import $ivy.`org.apache.spark::spark-hive:2.4.5`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create spark context and hiveContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spark-stubs\n",
      "Getting spark JARs\n",
      "Creating SparkSession\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a target=\"_blank\" href=\"http://192.168.3.29:4040\">Spark UI</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.hive.HiveContext\n",
       "\n",
       "//jupyter notebook way to create spark session\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@14f60cb\n",
       "\u001b[36msc\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mSparkContext\u001b[39m = org.apache.spark.SparkContext@164d251\n",
       "\u001b[36mhiveContext\u001b[39m: \u001b[32mHiveContext\u001b[39m = org.apache.spark.sql.hive.HiveContext@12fb98b"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.hive.HiveContext\n",
    "\n",
    "//jupyter notebook way to create spark session\n",
    "val spark = {\n",
    "  NotebookSparkSession.builder()\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    "}\n",
    "val sc = spark.sparkContext\n",
    "val hiveContext = new HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mhiveContext.implicits._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mhiveContext.sql\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hiveContext.implicits._\n",
    "import hiveContext.sql\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Hive table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdir\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"/home/pi/sparktest/data/transactionlogtest\"\u001b[39m\n",
       "\u001b[36mdata\u001b[39m: \u001b[32mjava\u001b[39m.\u001b[32mio\u001b[39m.\u001b[32mFile\u001b[39m = /home/pi/sparktest/data/transactionlogtest\n",
       "\u001b[36mpath\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"/home/pi/sparktest/data/transactionlogtest\"\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dir = \"/home/pi/sparktest/data/transactionlogtest\"\n",
    "val data = new java.io.File(dir)\n",
    "val path = data.getCanonicalPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31mjava.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.hive.conf.HiveConf$ConfVars\u001b[39m\n  org.apache.spark.sql.hive.HiveUtils$.formatTimeVarsForHiveClient(\u001b[32mHiveUtils.scala\u001b[39m:\u001b[32m192\u001b[39m)\n  org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(\u001b[32mHiveUtils.scala\u001b[39m:\u001b[32m285\u001b[39m)\n  org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(\u001b[32mHiveExternalCatalog.scala\u001b[39m:\u001b[32m66\u001b[39m)\n  org.apache.spark.sql.hive.HiveExternalCatalog.client(\u001b[32mHiveExternalCatalog.scala\u001b[39m:\u001b[32m65\u001b[39m)\n  org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(\u001b[32mHiveExternalCatalog.scala\u001b[39m:\u001b[32m215\u001b[39m)\n  scala.runtime.java8.JFunction0$mcZ$sp.apply(\u001b[32mJFunction0$mcZ$sp.java\u001b[39m:\u001b[32m23\u001b[39m)\n  org.apache.spark.sql.hive.HiveExternalCatalog.withClient(\u001b[32mHiveExternalCatalog.scala\u001b[39m:\u001b[32m97\u001b[39m)\n  org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(\u001b[32mHiveExternalCatalog.scala\u001b[39m:\u001b[32m215\u001b[39m)\n  org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(\u001b[32mSharedState.scala\u001b[39m:\u001b[32m114\u001b[39m)\n  org.apache.spark.sql.internal.SharedState.externalCatalog(\u001b[32mSharedState.scala\u001b[39m:\u001b[32m102\u001b[39m)\n  org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(\u001b[32mHiveSessionStateBuilder.scala\u001b[39m:\u001b[32m39\u001b[39m)\n  org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(\u001b[32mHiveSessionStateBuilder.scala\u001b[39m:\u001b[32m54\u001b[39m)\n  org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(\u001b[32mSessionCatalog.scala\u001b[39m:\u001b[32m90\u001b[39m)\n  org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(\u001b[32mSessionCatalog.scala\u001b[39m:\u001b[32m90\u001b[39m)\n  org.apache.spark.sql.catalyst.catalog.SessionCatalog.databaseExists(\u001b[32mSessionCatalog.scala\u001b[39m:\u001b[32m243\u001b[39m)\n  org.apache.spark.sql.catalyst.catalog.SessionCatalog.requireDbExists(\u001b[32mSessionCatalog.scala\u001b[39m:\u001b[32m177\u001b[39m)\n  org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(\u001b[32mSessionCatalog.scala\u001b[39m:\u001b[32m316\u001b[39m)\n  org.apache.spark.sql.execution.command.CreateTableCommand.run(\u001b[32mtables.scala\u001b[39m:\u001b[32m129\u001b[39m)\n  org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(\u001b[32mcommands.scala\u001b[39m:\u001b[32m70\u001b[39m)\n  org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(\u001b[32mcommands.scala\u001b[39m:\u001b[32m68\u001b[39m)\n  org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(\u001b[32mcommands.scala\u001b[39m:\u001b[32m79\u001b[39m)\n  org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(\u001b[32mDataset.scala\u001b[39m:\u001b[32m194\u001b[39m)\n  org.apache.spark.sql.Dataset.$anonfun$withAction$2(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3370\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m80\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m75\u001b[39m)\n  org.apache.spark.sql.Dataset.withAction(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3370\u001b[39m)\n  org.apache.spark.sql.Dataset.<init>(\u001b[32mDataset.scala\u001b[39m:\u001b[32m194\u001b[39m)\n  org.apache.spark.sql.Dataset$.ofRows(\u001b[32mDataset.scala\u001b[39m:\u001b[32m79\u001b[39m)\n  org.apache.spark.sql.SparkSession.sql(\u001b[32mSparkSession.scala\u001b[39m:\u001b[32m642\u001b[39m)\n  org.apache.spark.sql.SQLContext.sql(\u001b[32mSQLContext.scala\u001b[39m:\u001b[32m694\u001b[39m)\n  ammonite.$sess.cmd6$Helper.<init>(\u001b[32mcmd6.sc\u001b[39m:\u001b[32m14\u001b[39m)\n  ammonite.$sess.cmd6$.<init>(\u001b[32mcmd6.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd6$.<clinit>(\u001b[32mcmd6.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    }
   ],
   "source": [
    "sql(s\"\"\"\n",
    "    create external table if not exists transactionLog(\n",
    "        transactionName string,\n",
    "        requestName string,\n",
    "        requesterName string,\n",
    "        clientTransactionName string,\n",
    "        sessionId int,\n",
    "        transactionId int,\n",
    "        requestID int, \n",
    "        responseSize int, \n",
    "        transactionStatus int\n",
    "    )\n",
    "    row format delimited fields terminated by ','\n",
    "    location '$path'\n",
    "\"\"\").show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
